{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d948740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c74839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opts = FirefoxOptions()\n",
    "# opts.add_argument(\"--headless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4d44a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_scrape(driver):\n",
    "    # Add the page source to the variable `content`.\n",
    "    content = driver.page_source\n",
    "    # Load the contents of the page, its source, into BeautifulSoup \n",
    "    # class, which analyzes the HTML as a nested data structure and allows to select\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    download_button = soup.find_all(attrs={'class': '_9862b2b6a967 _b762866b4635 _6f058dbc4ded'})\n",
    "    \n",
    "    download_button[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f2a21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = uc.Chrome()\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ace771ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.studocu.com/en-us/search?categories=3&origin=header\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01099edf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ STARTING FIRST PAGE ------\n",
      "1\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "2\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "3\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "4\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "5\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "6\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "7\n",
      "8\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "9\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "10\n",
      "11\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "12\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "13\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "14\n",
      "https://www.studocu.com/en-us/search?categories=3&origin=header\n",
      "15\n",
      "Scraping https://www.studocu.com/en-us/document/university-of-kentucky/understanding-workplace-communication/chapter-10-lecture-notes/9535848 ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 40\u001b[0m\n\u001b[0;32m     35\u001b[0m             driver\u001b[38;5;241m.\u001b[39mget(link)\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;66;03m# driver.refresh()\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#           results.append(page_scrape(links_visited, driver))\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m             page \u001b[38;5;241m=\u001b[39m \u001b[43mpage_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscrape_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     43\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m page \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[25], line 10\u001b[0m, in \u001b[0;36mpage_scrape\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m      6\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m download_button \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_9862b2b6a967 _b762866b4635 _6f058dbc4ded\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 10\u001b[0m \u001b[43mdownload_button\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclick\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "# driver.refresh()\n",
    "links_visited = []\n",
    "results = []\n",
    "pages = 10_000\n",
    "num_scraped_already = 0\n",
    "\n",
    "elements = driver.find_elements(By.XPATH, \"//a[@href]\")\n",
    "links = [x.get_attribute(\"href\") for x in elements]\n",
    "links_seen = len(links)\n",
    "i_page = 0\n",
    "\n",
    "print(\"------ STARTING FIRST PAGE ------\")\n",
    "for i in range(pages):\n",
    "    if i != 0 and ((i_page) % links_seen == 0):\n",
    "        print(\"------ NEXT PAGE ------\")\n",
    "        driver.get(\"https://www.studocu.com/en-us/search?page=\" + str(i))\n",
    "        driver.refresh()\n",
    "        elements = driver.find_elements(By.XPATH, \"//a[@href]\")\n",
    "        links = [x.get_attribute(\"href\") for x in elements]\n",
    "        links_visited = []\n",
    "        links_seen = len(links)\n",
    "        i_page = 0\n",
    "\n",
    "    if i >= num_scraped_already and i_page<len(links):\n",
    "        link = links[i_page]\n",
    "        print(i_page + 1)\n",
    "        \n",
    "        if link not in links_visited:\n",
    "            pre_click = driver.current_url\n",
    "            links_visited.append(link)\n",
    "            if \"document\" in link and \"/document/upload\" not in link:\n",
    "                file_name = link.split(\"document\")[1]\n",
    "            \n",
    "                print(\"Scraping\", link, \"...\")\n",
    "                driver.get(link)\n",
    "                # driver.refresh()\n",
    "\n",
    "    #           results.append(page_scrape(links_visited, driver))\n",
    "\n",
    "                page = page_scrape(driver)\n",
    "\n",
    "                with open('scrape_' + '.txt', 'w') as f:\n",
    "                    if page is not None:\n",
    "                        f.write(page)\n",
    "            \n",
    "#             driver.refresh()\n",
    "            current = driver.current_url\n",
    "            if pre_click != current:\n",
    "                driver.back()\n",
    "                driver.refresh()\n",
    "            print(driver.current_url)\n",
    "\n",
    "            elements = driver.find_elements(By.XPATH, \"//a[@href]\")\n",
    "            links = [x.get_attribute(\"href\") for x in elements]\n",
    "    i_page = i_page + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4443fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4730 pages already scraped, please update the num_scraped_already value to 4729\n"
     ]
    }
   ],
   "source": [
    "print(i, \"pages already scraped, please update the num_scraped_already value to\", (i - 1))\n",
    "\n",
    "with open('num_scraped_alread.txt', 'w') as f:\n",
    "    f.write(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fb52145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "192\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "print(i_page)\n",
    "print(links_seen)\n",
    "print(len(links))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
